---
title: 2025 AI Engineering Reading List论文
date: 2026-01-14T14:33:34+08:00
draft: false
tags:
  - GPU
  - LLM
  - AI
  - DeepLearning
  - Tutorial
  - AGI
  - 幻觉
  - 强化学习
  - 模拟
  - 感知
  - 意识
  - paper
author: Ringi Lee
showToc: true
tocOpen: false
---

# 2025 AI Engineering Reading List 论文

## Section 1: Frontier LLMs

1. **[<span style="color: rgb(36,91,219); background-color: inherit">GPT1</span>](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), [<span style="color: rgb(36,91,219); background-color: inherit">GPT2</span>](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), [<span style="color: rgb(36,91,219); background-color: inherit">GPT3</span>](https://arxiv.org/abs/2005.14165), [<span style="color: rgb(36,91,219); background-color: inherit">Codex</span>](https://arxiv.org/abs/2107.03374), [<span style="color: rgb(36,91,219); background-color: inherit">InstructGPT</span>](https://arxiv.org/abs/2203.02155), [<span style="color: rgb(36,91,219); background-color: inherit">GPT4</span>](https://arxiv.org/abs/2303.08774) papers**. Self explanatory. [**<span style="color: rgb(36,91,219); background-color: inherit">GPT3.5</span>**](https://openai.com/index/chatgpt/), [**<span style="color: rgb(36,91,219); background-color: inherit">4o</span>**](https://openai.com/index/hello-gpt-4o/), [**<span style="color: rgb(36,91,219); background-color: inherit">o1</span>**](https://openai.com/index/introducing-openai-o1-preview/), **[<span style="color: rgb(36,91,219); background-color: inherit">o3</span>](https://openai.com/index/deliberative-alignment/), [<span style="color: rgb(36,91,219); background-color: inherit">GPT4.5</span>](https://www.interconnects.ai/p/gpt-45-not-a-frontier-model)** have launch events and system cards[<span style="color: rgb(36,91,219); background-color: inherit">(2)</span>](https://www.latent.space/p/2025-papers#footnote-2-153615976) instead.

2. **[<span style="color: rgb(36,91,219); background-color: inherit">Claude 3</span>](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf) / [<span style="color: rgb(36,91,219); background-color: inherit">4</span>](https://www-cdn.anthropic.com/4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf?s=09) and [<span style="color: rgb(36,91,219); background-color: inherit">Gemini 1</span>](https://arxiv.org/abs/2312.11805) / [<span style="color: rgb(36,91,219); background-color: inherit">2.5</span>](https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf)** to understand the leading labs. You should also know [<span style="color: rgb(36,91,219); background-color: inherit">Claude 3.5 Sonnet</span>](https://www.latent.space/p/claude-sonnet) and [<span style="color: rgb(36,91,219); background-color: inherit">Gemini 2.0 Flash</span>](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#gemini-2-0-flash)/[<span style="color: rgb(36,91,219); background-color: inherit">Flash Thinking</span>](https://ai.google.dev/gemini-api/docs/thinking-mode). Also [<span style="color: rgb(36,91,219); background-color: inherit">Gemma 2</span>](https://arxiv.org/abs/2408.00118) & [<span style="color: rgb(36,91,219); background-color: inherit">3</span>](https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf#page=20.92) (you can read the [<span style="color: rgb(36,91,219); background-color: inherit">PyTorch re-implementation</span>](https://news.ycombinator.com/item?id=44962059)).

3. **[<span style="color: rgb(36,91,219); background-color: inherit">LLaMA 1</span>](https://arxiv.org/abs/2302.13971), [<span style="color: rgb(36,91,219); background-color: inherit">Llama 2</span>](https://arxiv.org/abs/2307.09288), [<span style="color: rgb(36,91,219); background-color: inherit">Llama 3</span>](https://arxiv.org/abs/2407.21783)** **papers&#x20;**&#x74;o understand the leading open models. You can also view [<span style="color: rgb(36,91,219); background-color: inherit">Mistral 7B</span>](https://arxiv.org/abs/2310.06825), [<span style="color: rgb(36,91,219); background-color: inherit">Mixtral</span>](https://arxiv.org/abs/2401.04088) and [<span style="color: rgb(36,91,219); background-color: inherit">Pixtral</span>](https://arxiv.org/abs/2410.07073) as a branch on the Llama family tree. More recently China models have overtaken: [<span style="color: rgb(36,91,219); background-color: inherit">Kimi K-2</span>](https://moonshotai.github.io/Kimi-K2/) and [<span style="color: rgb(36,91,219); background-color: inherit">Qwen 3</span>](https://arxiv.org/abs/2505.09388).

4. **DeepSeek [<span style="color: rgb(36,91,219); background-color: inherit">V1</span>](https://arxiv.org/abs/2401.02954), [<span style="color: rgb(36,91,219); background-color: inherit">Coder</span>](https://arxiv.org/abs/2401.14196), [<span style="color: rgb(36,91,219); background-color: inherit">Math</span>](https://arxiv.org/abs/2402.03300)&#x20;**(esp [<span style="color: rgb(36,91,219); background-color: inherit">GRPO</span>](https://www.interconnects.ai/p/papers-im-reading-base-model-rl-grpo))**, [<span style="color: rgb(36,91,219); background-color: inherit">MoE</span>](https://arxiv.org/abs/2401.06066), [<span style="color: rgb(36,91,219); background-color: inherit">V2</span>](https://arxiv.org/abs/2405.04434),[<span style="color: rgb(36,91,219); background-color: inherit"> V3</span>](https://github.com/deepseek-ai/DeepSeek-V3), [<span style="color: rgb(36,91,219); background-color: inherit">R1</span>](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B) papers**.

5. **Post Training Survey Papers** like [<span style="color: rgb(36,91,219); background-color: inherit">this MBZUAI one</span>](https://youtu.be/XCgWoLu-zK8), [<span style="color: rgb(36,91,219); background-color: inherit">this one</span>](https://x.com/zainhasan6/status/1899541155924046006?s=46), and others.

You can both use and learn a lot from other LLMs, this is a vast topic. Bonus picks:

- In particular, BERTs are underrated as workhorse classification models - see [<span style="color: rgb(36,91,219); background-color: inherit">ModernBERT</span>](https://buttondown.com/ainews/archive/ainews-modernbert-small-new-retrieverclassifier/) / [<span style="color: rgb(36,91,219); background-color: inherit">NeoBERT</span>](https://arxiv.org/abs/2502.19587) for the state of the art, and [<span style="color: rgb(36,91,219); background-color: inherit">ColBERT for applications</span>](https://www.answer.ai/posts/colbert-pooling.html).

- (1.5hr video) [<span style="color: rgb(36,91,219); background-color: inherit">Compare architectures</span>](https://x.com/rasbt/status/1965798055141429523?s=46) of the various open models from [<span style="color: rgb(36,91,219); background-color: inherit">Ahead of AI</span>](https://open.substack.com/pub/sebastianraschka).

- Honorable mentions of LLMs to know: AI2 ([<span style="color: rgb(36,91,219); background-color: inherit">Olmo</span>](https://arxiv.org/abs/2402.00838), [<span style="color: rgb(36,91,219); background-color: inherit">Molmo</span>](https://arxiv.org/abs/2409.17146), [<span style="color: rgb(36,91,219); background-color: inherit">OlmOE</span>](https://arxiv.org/abs/2409.02060), [<span style="color: rgb(36,91,219); background-color: inherit">Tülu</span>](https://allenai.org/blog/tulu-3-technical) 3, [<span style="color: rgb(36,91,219); background-color: inherit">Olmo 2</span>](https://x.com/soldni/status/1875266934943649808?s=46)), [<span style="color: rgb(36,91,219); background-color: inherit">Grok</span>](https://github.com/xai-org/grok-1) (esp [<span style="color: rgb(36,91,219); background-color: inherit">Grok 3</span>](https://x.ai/news/grok-3)), [<span style="color: rgb(36,91,219); background-color: inherit">Amazon Nova</span>](https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/), [<span style="color: rgb(36,91,219); background-color: inherit">Yi</span>](https://www.wired.com/story/chinese-startup-01-ai-is-winning-the-open-source-ai-race/), [<span style="color: rgb(36,91,219); background-color: inherit">Reka</span>](https://www.latent.space/p/yitay), [<span style="color: rgb(36,91,219); background-color: inherit">Cohere</span>](https://cohere.com/command), [<span style="color: rgb(36,91,219); background-color: inherit">Nemotron</span>](https://buttondown.com/ainews/archive/ainews-to-be-named-2748/), [<span style="color: rgb(36,91,219); background-color: inherit">Microsoft Phi</span>](https://arxiv.org/abs/2412.08905), [<span style="color: rgb(36,91,219); background-color: inherit">HuggingFace SmolLM</span>](https://www.latent.space/p/2024-open-models) (esp [<span style="color: rgb(36,91,219); background-color: inherit">SmolLM 3</span>](https://huggingface.co/blog/smollm3)), [<span style="color: rgb(36,91,219); background-color: inherit">Apple Intelligence</span>](https://arxiv.org/abs/2407.21075) - mostly lower in ranking or lack papers.

- Research to know: If time allows, we recommend the **Scaling Laws** literature: [<span style="color: rgb(36,91,219); background-color: inherit">Kaplan</span>](http://arxiv.org/abs/2001.08361), [<span style="color: rgb(36,91,219); background-color: inherit">Chinchilla</span>](https://arxiv.org/abs/2203.15556), [<span style="color: rgb(36,91,219); background-color: inherit">Emergence</span>](https://arxiv.org/abs/2206.07682) / [<span style="color: rgb(36,91,219); background-color: inherit">Mirage</span>](https://arxiv.org/abs/2304.15004), [<span style="color: rgb(36,91,219); background-color: inherit">Post-Chinchilla laws</span>](https://arxiv.org/abs/2401.00448).[<span style="color: rgb(36,91,219); background-color: inherit">(3)</span>](https://www.latent.space/p/2025-papers#footnote-3-153615976) As of July 2025, [<span style="color: rgb(36,91,219); background-color: inherit">Kimi K-2/Moonshot</span>](https://moonshotai.github.io/Kimi-K2/) and [<span style="color: rgb(36,91,219); background-color: inherit">Muon</span>](https://arxiv.org/abs/2502.16982) is also strongly recommended.

- In 2025, the frontier (o1, [<span style="color: rgb(36,91,219); background-color: inherit">o3</span>](https://en.wikipedia.org/wiki/OpenAI_o3), [<span style="color: rgb(36,91,219); background-color: inherit">R1</span>](https://api-docs.deepseek.com/news/news1120), [<span style="color: rgb(36,91,219); background-color: inherit">QwQ</span>](https://qwenlm.github.io/blog/qwq-32b-preview/)/[<span style="color: rgb(36,91,219); background-color: inherit">QVQ</span>](https://qwenlm.github.io/blog/qvq-72b-preview/), [<span style="color: rgb(36,91,219); background-color: inherit">f1</span>](https://fireworks.ai/blog/fireworks-compound-ai-system-f1)) will be very much dominated by **reasoning models**, where [<span style="color: rgb(36,91,219); background-color: inherit">Sebastian Raschka</span>](https://magazine.sebastianraschka.com/p/understanding-reasoning-llms) currently has the best literature review. The basic knowledge is [<span style="color: rgb(36,91,219); background-color: inherit">Let’s Verify Step By Step</span>](https://arxiv.org/abs/2305.20050)[<span style="color: rgb(36,91,219); background-color: inherit">(4)</span>](https://www.latent.space/p/2025-papers#footnote-4-153615976), [<span style="color: rgb(36,91,219); background-color: inherit">s1</span>](https://arxiv.org/abs/2501.19393), [<span style="color: rgb(36,91,219); background-color: inherit">STaR</span>](https://www.youtube.com/watch?v=Y5-FeaFOEFM) ([<span style="color: rgb(36,91,219); background-color: inherit">core to xAI</span>](https://x.com/shaneguML/status/1944558999342661963)), and [<span style="color: rgb(36,91,219); background-color: inherit">Noam Brown’s talks/podcasts</span>](https://www.youtube.com/live/Gr_eYXdHFis). Most practical knowledge is [<span style="color: rgb(36,91,219); background-color: inherit">accumulated by outsiders</span>](https://hn.algolia.com/?dateRange=all&page=0&prefix=false&query=o1&sort=byPopularity&type=story) ([<span style="color: rgb(36,91,219); background-color: inherit">LS talk</span>](https://www.youtube.com/watch?v=skT89EvIjrc&t=68s)) and tweets.

-

## Section 2: Benchmarks and Evals

1. **[<span style="color: rgb(36,91,219); background-color: inherit">MMLU</span>](https://arxiv.org/abs/2009.03300) paper** - the main **knowledge** benchmark, next to [**<span style="color: rgb(36,91,219); background-color: inherit">GPQA</span>**](https://arxiv.org/abs/2311.12022) and [<span style="color: rgb(36,91,219); background-color: inherit">BIG-Bench</span>](https://arxiv.org/abs/2206.04615). In 2025 frontier labs use [**<span style="color: rgb(36,91,219); background-color: inherit">MMLU Pro</span>**](https://arxiv.org/abs/2406.01574), **[<span style="color: rgb(36,91,219); background-color: inherit">GPQA Diamond</span>](https://arxiv.org/abs/2311.12022), and [<span style="color: rgb(36,91,219); background-color: inherit">BIG-Bench Hard</span>](https://arxiv.org/abs/2210.09261).**

2. [**<span style="color: rgb(36,91,219); background-color: inherit">MRCR</span>**](https://arxiv.org/html/2409.12640v2)[<span style="color: rgb(36,91,219); background-color: inherit">(5)</span>](https://www.latent.space/p/2025-papers#footnote-5-153615976)**&#x20;paper** - evaluating **long context**, [<span style="color: rgb(36,91,219); background-color: inherit">used by OpenAI</span>](https://www.latent.space/p/quasar) over [<span style="color: rgb(36,91,219); background-color: inherit">MuSR</span>](https://arxiv.org/abs/2310.16049), [<span style="color: rgb(36,91,219); background-color: inherit">LongBench</span>](https://arxiv.org/abs/2412.15204), [<span style="color: rgb(36,91,219); background-color: inherit">BABILong</span>](https://arxiv.org/abs/2406.10149), and [<span style="color: rgb(36,91,219); background-color: inherit">RULER</span>](https://www.latent.space/p/gradient). Fixing the overused [<span style="color: rgb(36,91,219); background-color: inherit">Needle in a Haystack</span>](https://github.com/gkamradt/LLMTest_NeedleInAHaystack).

3. **[<span style="color: rgb(36,91,219); background-color: inherit">MATH </span>](https://arxiv.org/abs/2103.03874)paper** - a compilation of **math competition problems**. Frontier labs focus on [<span style="color: rgb(36,91,219); background-color: inherit">FrontierMath</span>](https://arxiv.org/abs/2411.04872), [<span style="color: rgb(36,91,219); background-color: inherit">AMO</span>](https://x.com/jack_w_rae/status/1924897579122491523) and subsets of MATH: MATH level 5, [<span style="color: rgb(36,91,219); background-color: inherit">AIME</span>](https://www.kaggle.com/datasets/hemishveeraboina/aime-problem-set-1983-2024), [<span style="color: rgb(36,91,219); background-color: inherit">AMC10/AMC12</span>](https://github.com/ryanrudes/amc).

4. **[<span style="color: rgb(36,91,219); background-color: inherit">IFEval</span>](https://arxiv.org/abs/2311.07911) paper&#x20;**- the leading **instruction following&#x20;**&#x65;val. See also [<span style="color: rgb(36,91,219); background-color: inherit">Facebook’s</span>](https://github.com/facebookresearch/Multi-IF) [<span style="color: rgb(36,91,219); background-color: inherit">Multi-IF</span>](https://arxiv.org/abs/2410.15553), [<span style="color: rgb(36,91,219); background-color: inherit">COLLIE</span>](https://collie-benchmark.github.io/) and [<span style="color: rgb(36,91,219); background-color: inherit">Scale MultiChallenge</span>](https://arxiv.org/abs/2501.17399), which has now overtaken [<span style="color: rgb(36,91,219); background-color: inherit">MT-Bench</span>](https://huggingface.co/spaces/lmsys/mt-bench).

5. **[<span style="color: rgb(36,91,219); background-color: inherit">ARC AGI</span>](https://arcprize.org/arc) challenge** - a famous **abstract reasoning** “IQ test” benchmark that has lasted far longer than many quickly saturated benchmarks.

This is a moving and [<span style="color: rgb(36,91,219); background-color: inherit">often gamed</span>](https://x.com/kalomaze/status/1923983739299955064) target - [<span style="color: rgb(36,91,219); background-color: inherit">May 2025 update</span>](https://x.com/scaling01/status/1919092778648408363) here. We covered many of these in [<span style="color: rgb(36,91,219); background-color: inherit">Benchmarks 101</span>](https://www.latent.space/p/benchmarks-101) and [<span style="color: rgb(36,91,219); background-color: inherit">Benchmarks 201</span>](https://www.latent.space/p/benchmarks-201), while our [<span style="color: rgb(36,91,219); background-color: inherit">Carlini</span>](https://www.latent.space/p/carlini), [<span style="color: rgb(36,91,219); background-color: inherit">LMArena</span>](https://www.latent.space/p/lmarena) ([<span style="color: rgb(36,91,219); background-color: inherit">warning</span>](https://x.com/karpathy/status/1917546757929722115)!), and [<span style="color: rgb(36,91,219); background-color: inherit">Braintrust</span>](https://www.latent.space/p/braintrust) covered private, arena, and product evals (read [<span style="color: rgb(36,91,219); background-color: inherit">LLM-as-Judge</span>](https://hamel.dev/blog/posts/llm-judge/) - and [<span style="color: rgb(36,91,219); background-color: inherit">Rubrics</span>](https://x.com/iscienceluvr/status/1948235609190867054?s=46) -and[<span style="color: rgb(36,91,219); background-color: inherit"> Applied LLMs essay</span>](https://applied-llms.org/#evaluation-monitoring)). Benchmarks are linked to [<span style="color: rgb(36,91,219); background-color: inherit">Datasets</span>](https://www.latent.space/p/datasets-101).

The bigger picture is that benchmarks saturate increasingly quickly, and there is a sense that [<span style="color: rgb(36,91,219); background-color: inherit">the whole approach is getting dated</span>](https://ysymyth.github.io/The-Second-Half/) and [<span style="color: rgb(36,91,219); background-color: inherit">new approaches are needed</span>](https://research.trychroma.com/generative-benchmarking).

![](images/progress.jpeg)

## Section 3: Prompting, ICL & Chain of Thought

> **\*Note**: The GPT3 paper (“Language Models are Few-Shot Learners”) should already have introduced In-Context Learning (ICL) - a close cousin of prompting. We also consider [<span style="color: rgb(36,91,219); background-color: inherit">prompt injections</span>](https://www.latent.space/i/93381455/what-is-prompt-injection) required knowledge — [<span style="color: rgb(36,91,219); background-color: inherit">Lilian Weng</span>](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/), [<span style="color: rgb(36,91,219); background-color: inherit">Simon W</span>](https://simonwillison.net/series/prompt-injection/).\*

1. [**<span style="color: rgb(36,91,219); background-color: inherit">The Prompt Report</span>**](https://arxiv.org/abs/2406.06608) **paper** - a survey of prompting papers ([<span style="color: rgb(36,91,219); background-color: inherit">podcast</span>](https://www.latent.space/p/learn-prompting)).

2. **[<span style="color: rgb(36,91,219); background-color: inherit">Chain-of-Thought</span>](https://arxiv.org/abs/2201.11903) paper&#x20;**- one of multiple claimants to popularizing **Chain of Thought**, along with [**<span style="color: rgb(36,91,219); background-color: inherit">Scratchpads</span>**](https://arxiv.org/abs/2112.00114) and [**<span style="color: rgb(36,91,219); background-color: inherit">Let’s Think Step By Step</span>**](https://arxiv.org/abs/2205.11916)

3. **[<span style="color: rgb(36,91,219); background-color: inherit">Tree of Thought</span>](https://arxiv.org/abs/2305.10601) paper** -**&#x20;**&#x69;ntroducing **lookaheads** and **backtracking** ([<span style="color: rgb(36,91,219); background-color: inherit">podcast</span>](https://www.latent.space/p/shunyu))

4. **[<span style="color: rgb(36,91,219); background-color: inherit">Prompt Tuning</span>](https://aclanthology.org/2021.emnlp-main.243/) paper -** you may not need prompts - if you can do [<span style="color: rgb(36,91,219); background-color: inherit">Prefix-Tuning</span>](https://arxiv.org/abs/2101.00190), [<span style="color: rgb(36,91,219); background-color: inherit">adjust decoding</span>](https://arxiv.org/abs/2402.10200) (say [<span style="color: rgb(36,91,219); background-color: inherit">via entropy</span>](https://github.com/xjdr-alt/entropix)), or [<span style="color: rgb(36,91,219); background-color: inherit">representation engineering</span>](https://vgel.me/posts/representation-engineering/)

5. **[<span style="color: rgb(36,91,219); background-color: inherit">Automatic Prompt Engineering</span>](https://arxiv.org/abs/2211.01910) paper** - it is increasingly obvious that **humans are terrible zero-shot prompters** and _prompting itself&#x20;_&#x63;an be enhanced by LLMs. The most notable implementation of this is in the [<span style="color: rgb(36,91,219); background-color: inherit">DSPy paper</span>](https://arxiv.org/abs/2310.03714)/framework.

More survey papers on [<span style="color: rgb(36,91,219); background-color: inherit">reasoning</span>](https://x.com/denny_zhou/status/1948499173986201915) ([<span style="color: rgb(36,91,219); background-color: inherit">slides</span>](https://dennyzhou.github.io/LLM-Reasoning-Stanford-CS-25.pdf)). Prompting and [<span style="color: rgb(36,91,219); background-color: inherit">Context Engineering</span>](https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html) is one area where reading disparate papers may not be as useful as having more practical guides - we recommend [<span style="color: rgb(36,91,219); background-color: inherit">Lilian Weng</span>](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/), [<span style="color: rgb(36,91,219); background-color: inherit">Eugene Yan</span>](https://eugeneyan.com/writing/prompting/), and [<span style="color: rgb(36,91,219); background-color: inherit">Anthropic’s Prompt Engineering Tutorial</span>](https://github.com/anthropics/prompt-eng-interactive-tutorial) and [<span style="color: rgb(36,91,219); background-color: inherit">AI Engineer Workshop</span>](https://www.youtube.com/watch?v=hkhDdcM5V94).

## Section 4: Retrieval Augmented Generation

1. **[<span style="color: rgb(36,91,219); background-color: inherit">Introduction to Information Retrieval</span>](https://nlp.stanford.edu/IR-book/information-retrieval-book.html)&#x20;**- a bit unfair to recommend a book, but we are trying to make the point that RAG is an IR problem and [<span style="color: rgb(36,91,219); background-color: inherit">IR has a 60 year history</span>](https://en.wikipedia.org/wiki/Information_retrieval#History) that includes [<span style="color: rgb(36,91,219); background-color: inherit">TF-IDF</span>](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), [<span style="color: rgb(36,91,219); background-color: inherit">BM25</span>](https://en.wikipedia.org/wiki/Okapi_BM25), [<span style="color: rgb(36,91,219); background-color: inherit">FAISS</span>](https://github.com/facebookresearch/faiss), [<span style="color: rgb(36,91,219); background-color: inherit">HNSW</span>](https://arxiv.org/abs/1603.09320) and other “boring” techniques.

2. **2020 [<span style="color: rgb(36,91,219); background-color: inherit">Meta RAG</span>](https://arxiv.org/abs/2005.11401) paper** - which coined the term. The original authors have started Contextual and have [<span style="color: rgb(36,91,219); background-color: inherit">coined RAG 2.0</span>](https://contextual.ai/introducing-rag2/). Modern “table stakes” for RAG — [<span style="color: rgb(36,91,219); background-color: inherit">HyDE</span>](https://docs.llamaindex.ai/en/stable/optimizing/advanced_retrieval/query_transformations/), [<span style="color: rgb(36,91,219); background-color: inherit">chunking</span>](https://research.trychroma.com/evaluating-chunking), [<span style="color: rgb(36,91,219); background-color: inherit">rerankers</span>](https://cohere.com/blog/rerank-3pt5), [<span style="color: rgb(36,91,219); background-color: inherit">multimodal data</span>](https://www.youtube.com/watch?v=i2vBaFzCEJw) are [<span style="color: rgb(36,91,219); background-color: inherit">better</span>](https://www.youtube.com/watch?v=TRjq7t2Ms5I&t=152s) [<span style="color: rgb(36,91,219); background-color: inherit">presented</span>](https://www.youtube.com/watch?v=FDEmbYPgG-s) [<span style="color: rgb(36,91,219); background-color: inherit">elsewhere</span>](https://www.youtube.com/watch?v=DId2KP8Ykz4).

3. **[<span style="color: rgb(36,91,219); background-color: inherit">MTEB</span>](https://arxiv.org/abs/2210.07316) paper&#x20;**- [<span style="color: rgb(36,91,219); background-color: inherit">known overfitting</span>](https://news.ycombinator.com/item?id=42504379) that [<span style="color: rgb(36,91,219); background-color: inherit">its author considers it dead</span>](https://x.com/Nils_Reimers/status/1870812625505849849), but still de-facto benchmark. Many embeddings have papers - pick your poison - [<span style="color: rgb(36,91,219); background-color: inherit">SentenceTransformers</span>](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2), [**<span style="color: rgb(36,91,219); background-color: inherit">OpenAI</span>**<span style="color: rgb(36,91,219); background-color: inherit">, </span>**<span style="color: rgb(36,91,219); background-color: inherit">Nomic Embed, Jina v3, cde-small-v1</span>**](https://www.youtube.com/watch?v=VIqXNRsRRQo)**,&#x20;**[<span style="color: rgb(36,91,219); background-color: inherit">ModernBERT Embed</span>](https://x.com/zach_nussbaum/status/1873813021786767699?s=46&t=tMWvmS3OL3Ssg0b9lKvp4Q) - with [<span style="color: rgb(36,91,219); background-color: inherit">Matryoshka embeddings</span>](https://huggingface.co/blog/matryoshka) increasingly standard.

4. **[<span style="color: rgb(36,91,219); background-color: inherit">GraphRAG</span>](https://arxiv.org/pdf/2404.16130) paper** - [<span style="color: rgb(36,91,219); background-color: inherit">Microsoft’s</span>](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/?utm_source=ainews&utm_medium=email&utm_campaign=ainews-graphrag) take on adding knowledge graphs to RAG, [<span style="color: rgb(36,91,219); background-color: inherit">now open sourced</span>](https://buttondown.com/ainews/archive/ainews-graphrag/). One of the [<span style="color: rgb(36,91,219); background-color: inherit">most popular trends in RAG</span>](https://www.youtube.com/watch?v=knDDGYHnnSI) in 2024, alongside of [<span style="color: rgb(36,91,219); background-color: inherit">ColBERT</span>](https://github.com/stanford-futuredata/ColBERT)/ColPali/ColQwen (more in the Vision section).

5. **[<span style="color: rgb(36,91,219); background-color: inherit">RAGAS</span>](https://arxiv.org/abs/2309.15217) paper -&#x20;**&#x74;he simple RAG eval [<span style="color: rgb(36,91,219); background-color: inherit">recommended by OpenAI</span>](https://x.com/swyx/status/1724490887147978793). See als&#x6F;**&#x20;[<span style="color: rgb(36,91,219); background-color: inherit">Nvidia FACTS framework</span>](https://arxiv.org/abs/2407.07858v1)&#x20;**&#x61;nd **[<span style="color: rgb(36,91,219); background-color: inherit">Extrinsic Hallucinations in LLMs</span>](https://lilianweng.github.io/posts/2024-07-07-hallucination/)&#x20;**- Lilian Weng’s survey of causes/evals for hallucinations (see also [<span style="color: rgb(36,91,219); background-color: inherit">Jason Wei on recall vs precision</span>](https://x.com/_jasonwei/status/1871285864690815053)).

RAG is the bread and butter of AI Engineering at work in 2024, so there are a LOT of industry resources and practical experience you will be expected to have. [<span style="color: rgb(36,91,219); background-color: inherit">LlamaIndex</span>](https://docs.llamaindex.ai/en/stable/understanding/rag/) ([<span style="color: rgb(36,91,219); background-color: inherit">course</span>](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/)) and [<span style="color: rgb(36,91,219); background-color: inherit">LangChain</span>](https://python.langchain.com/docs/tutorials/rag/) ([<span style="color: rgb(36,91,219); background-color: inherit">video</span>](https://www.youtube.com/watch?v=wd7TZ4w1mSw)) have perhaps invested the most in educational resources. You should also be familiar with the perennial [<span style="color: rgb(36,91,219); background-color: inherit">RAG vs Long Context</span>](https://arxiv.org/abs/2407.16833) debate.

## Section 5: Agents

1. **[<span style="color: rgb(36,91,219); background-color: inherit">SWE-Bench</span>](https://arxiv.org/abs/2310.06770)&#x20;**([<span style="color: rgb(36,91,219); background-color: inherit">our pod</span>](https://www.latent.space/p/iclr-2024-benchmarks-agents?utm_source=publication-search#%C2%A7section-b-benchmarks)) & [<span style="color: rgb(36,91,219); background-color: inherit">SWE-Lancer</span>](https://openai.com/index/swe-lancer/) - after [<span style="color: rgb(36,91,219); background-color: inherit">adoption by Anthropic</span>](https://www.latent.space/p/claude-sonnet), Devin and [<span style="color: rgb(36,91,219); background-color: inherit">OpenAI</span>](https://openai.com/index/introducing-swe-bench-verified/), probably the highest profile agent benchmark[<span style="color: rgb(36,91,219); background-color: inherit">(6)</span>](https://www.latent.space/p/2025-papers#footnote-6-153615976) today (vs [<span style="color: rgb(36,91,219); background-color: inherit">WebArena</span>](https://github.com/web-arena-x/webarena) or [<span style="color: rgb(36,91,219); background-color: inherit">SWE-Gym</span>](https://x.com/jiayi_pirate/status/1871249410128322856)). Technically a coding benchmark, but more a test of agents than raw LLMs. See also [<span style="color: rgb(36,91,219); background-color: inherit">SWE-Agent</span>](https://arxiv.org/abs/2405.15793), [<span style="color: rgb(36,91,219); background-color: inherit">SWE-Bench Multimodal</span>](https://arxiv.org/abs/2410.03859) and the [<span style="color: rgb(36,91,219); background-color: inherit">Konwinski Prize</span>](https://kprize.ai/). For Tool-Agent-User interaction there is [<span style="color: rgb(36,91,219); background-color: inherit">TauBench</span>](https://arxiv.org/abs/2406.12045) for Airlines and Retail and [<span style="color: rgb(36,91,219); background-color: inherit">GAIA</span>](https://arxiv.org/abs/2311.12983).

2. **[<span style="color: rgb(36,91,219); background-color: inherit">ReAct</span>](https://arxiv.org/abs/2210.03629) paper&#x20;**([<span style="color: rgb(36,91,219); background-color: inherit">our podcast</span>](https://www.latent.space/p/shunyu)) - ReAct started a long line of research on **tool using and function calling LLMs,&#x20;**&#x69;ncluding [<span style="color: rgb(36,91,219); background-color: inherit">Gorilla</span>](https://gorilla.cs.berkeley.edu/) and the [<span style="color: rgb(36,91,219); background-color: inherit">BFCL Leaderboard</span>](https://gorilla.cs.berkeley.edu/leaderboard.html). Of historical interest - [<span style="color: rgb(36,91,219); background-color: inherit">Toolformer</span>](https://arxiv.org/abs/2302.04761) and [<span style="color: rgb(36,91,219); background-color: inherit">HuggingGPT</span>](https://arxiv.org/abs/2303.17580).

3. **[<span style="color: rgb(36,91,219); background-color: inherit">MemGPT</span>](https://arxiv.org/abs/2310.08560) paper** - one of [<span style="color: rgb(36,91,219); background-color: inherit">many notable approaches</span>](https://x.com/swyx/status/1915128966203236571) to emulating long running agent memory, adopted by [<span style="color: rgb(36,91,219); background-color: inherit">ChatGPT</span>](https://openai.com/index/memory-and-new-controls-for-chatgpt/) and [<span style="color: rgb(36,91,219); background-color: inherit">LangGraph</span>](https://langchain-ai.github.io/langgraph/concepts/memory/#episodic-memory). Versions of these are reinvented in every agent system from [<span style="color: rgb(36,91,219); background-color: inherit">MetaGPT</span>](https://arxiv.org/abs/2308.00352) to [<span style="color: rgb(36,91,219); background-color: inherit">AutoGen</span>](https://arxiv.org/abs/2308.08155) to [<span style="color: rgb(36,91,219); background-color: inherit">Smallville</span>](https://github.com/joonspk-research/generative_agents).

4. **[<span style="color: rgb(36,91,219); background-color: inherit">Voyager</span>](https://arxiv.org/abs/2305.16291) paper** - Nvidia’s take on 3 [<span style="color: rgb(36,91,219); background-color: inherit">cognitive architecture</span>](https://arxiv.org/abs/2309.02427) components (**curriculum, skill library, sandbox**) to improve performance. More abstractly, skill library/curriculum can be abstracted as a form of [<span style="color: rgb(36,91,219); background-color: inherit">Agent Workflow Memory</span>](https://arxiv.org/abs/2409.07429).

5. **Anthropic on [<span style="color: rgb(36,91,219); background-color: inherit">Building Effective Agents</span>](https://www.anthropic.com/research/building-effective-agents)&#x20;**([<span style="color: rgb(36,91,219); background-color: inherit">talk version</span>](https://x.com/aiDotEngineer/status/1908230651985485955)) - just a great state-of-2024 recap that focuses on the importance of **chaining**, **routing, parallelization, orchestration, evaluation, and optimization.** See also [<span style="color: rgb(36,91,219); background-color: inherit">Lilian Weng’s Agents</span>](https://lilianweng.github.io/posts/2023-06-23-agent/) (ex OpenAI), [<span style="color: rgb(36,91,219); background-color: inherit">Shunyu Yao on LLM Agents</span>](https://www.latent.space/p/shunyu) (now at OpenAI) and [<span style="color: rgb(36,91,219); background-color: inherit">Chip Huyen’s Agents</span>](https://huyenchip.com//2025/01/07/agents.html).

We covered many of [<span style="color: rgb(36,91,219); background-color: inherit">the 2024 SOTA agent designs at NeurIPS</span>](https://www.latent.space/p/2024-agents), and you can find more readings in [<span style="color: rgb(36,91,219); background-color: inherit">the UC Berkeley LLM Agents MOOC</span>](https://llmagents-learning.org/f24). Note that we skipped bikeshedding agent definitions, but if you really need one, you could [<span style="color: rgb(36,91,219); background-color: inherit">use mine</span>](https://www.youtube.com/watch?v=wnsZ7DuqYp0).

## Section 6: Code Generation

1. **[<span style="color: rgb(36,91,219); background-color: inherit">The Stack paper</span>](https://arxiv.org/abs/2211.15533)&#x20;**- the original open dataset twin of The Pile focused on code, starting a great lineage of open codegen work from [<span style="color: rgb(36,91,219); background-color: inherit">The Stack v2</span>](https://huggingface.co/datasets/bigcode/the-stack-v2) to [<span style="color: rgb(36,91,219); background-color: inherit">StarCoder</span>](https://arxiv.org/abs/2402.19173).

2. **Open Code Model papers** - choose from [<span style="color: rgb(36,91,219); background-color: inherit">DeepSeek-Coder</span>](https://arxiv.org/abs/2401.14196), [<span style="color: rgb(36,91,219); background-color: inherit">Qwen2.5-Coder</span>](https://arxiv.org/abs/2409.12186), or [<span style="color: rgb(36,91,219); background-color: inherit">CodeLlama</span>](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/). Many regard [<span style="color: rgb(36,91,219); background-color: inherit">3.5 Sonnet as the best code model</span>](https://www.latent.space/p/claude-sonnet) but it has no paper.

3. [**<span style="color: rgb(36,91,219); background-color: inherit">HumanEval/Codex paper</span>**](https://arxiv.org/abs/2107.03374) - This is a saturated benchmark, but is required knowledge for the code domain. SWE-Bench is more famous for coding now, but is expensive/evals agents rather than models. Modern replacements include [<span style="color: rgb(36,91,219); background-color: inherit">Aider</span>](https://aider.chat/docs/leaderboards/), [<span style="color: rgb(36,91,219); background-color: inherit">Codeforces</span>](https://arxiv.org/abs/2312.02143), [<span style="color: rgb(36,91,219); background-color: inherit">IOI</span>](https://arxiv.org/pdf/2502.06807v1), [<span style="color: rgb(36,91,219); background-color: inherit">BigCodeBench</span>](https://huggingface.co/spaces/bigcode/bigcodebench-leaderboard), [<span style="color: rgb(36,91,219); background-color: inherit">LiveCodeBench</span>](https://livecodebench.github.io/) and [<span style="color: rgb(36,91,219); background-color: inherit">SciCode</span>](https://buttondown.com/ainews/archive/ainews-to-be-named-5745/).

4. **[<span style="color: rgb(36,91,219); background-color: inherit">AlphaCodeium paper</span>](https://arxiv.org/abs/2401.08500)&#x20;**- Google published [<span style="color: rgb(36,91,219); background-color: inherit">AlphaCode</span>](https://news.ycombinator.com/item?id=34020025) and [<span style="color: rgb(36,91,219); background-color: inherit">AlphaCode2</span>](https://x.com/RemiLeblond/status/1732419456272318614) which did very well on programming problems, but here is one way **Flow Engineering&#x20;**&#x63;an add a lot more performance to any given base model.

5. **[<span style="color: rgb(36,91,219); background-color: inherit">CriticGPT</span>](https://criticgpt.org/criticgpt-openai/) paper** - LLMs are [<span style="color: rgb(36,91,219); background-color: inherit">known</span>](https://arxiv.org/abs/2412.15004v1) to generate code that can have security issues. OpenAI trained CriticGPT to spot them, and Anthropic [<span style="color: rgb(36,91,219); background-color: inherit">uses SAEs to identify LLM features</span>](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#safety-relevant-code) that cause this, but it is a problem you should be aware of.

CodeGen is another field where much of the frontier has moved from research to industry and [<span style="color: rgb(36,91,219); background-color: inherit">practical engineering advice on codegen</span>](https://www.youtube.com/watch?v=Ve-akpov78Q) and [<span style="color: rgb(36,91,219); background-color: inherit">code agents like Devin</span>](https://www.youtube.com/watch?v=T7NWjoD_OuY&t=8s) are only found in industry blogposts and talks rather than research papers.

## Section 7: Vision

- **Non-LLM Vision work** is still important: e.g. the **[<span style="color: rgb(36,91,219); background-color: inherit">YOLO</span>](https://arxiv.org/abs/1506.02640) paper&#x20;**(now [<span style="color: rgb(36,91,219); background-color: inherit">up to v11</span>](https://github.com/ultralytics/ultralytics), but [<span style="color: rgb(36,91,219); background-color: inherit">mind the lineage</span>](https://news.ycombinator.com/item?id=42352342)), but increasingly transformers like [<span style="color: rgb(36,91,219); background-color: inherit">DETRs Beat YOLOs</span>](https://arxiv.org/abs/2304.08069) too.

- **[<span style="color: rgb(36,91,219); background-color: inherit">CLIP</span>](https://arxiv.org/abs/2103.00020) paper&#x20;**- the first successful [<span style="color: rgb(36,91,219); background-color: inherit">ViT</span>](https://arxiv.org/abs/2010.11929) from Alec Radford. These days, superceded by [<span style="color: rgb(36,91,219); background-color: inherit">BLIP</span>](https://arxiv.org/abs/2201.12086)/[<span style="color: rgb(36,91,219); background-color: inherit">BLIP2</span>](https://arxiv.org/abs/2301.12597) or [<span style="color: rgb(36,91,219); background-color: inherit">SigLIP/PaliGemma</span>](https://www.latent.space/i/152857207/part-vision), but still required to know.

- **[<span style="color: rgb(36,91,219); background-color: inherit">MMVP benchmark</span>](https://arxiv.org/abs/2401.06209)&#x20;**([<span style="color: rgb(36,91,219); background-color: inherit">LS Live</span>](https://www.latent.space/p/2024-vision))- quantifies issues with CLIP. Multimodal versions of MMLU ([<span style="color: rgb(36,91,219); background-color: inherit">MMMU</span>](https://arxiv.org/abs/2311.16502)), and [<span style="color: rgb(36,91,219); background-color: inherit">SWE-Bench</span>](https://arxiv.org/abs/2410.03859) do exist. See also [<span style="color: rgb(36,91,219); background-color: inherit">MathVista</span>](https://github.com/lupantech/MathVista) and [<span style="color: rgb(36,91,219); background-color: inherit">CharXiv</span>](https://charxiv.github.io/).

- **[<span style="color: rgb(36,91,219); background-color: inherit">Segment Anything Model </span>](https://arxiv.org/abs/2304.02643)and [<span style="color: rgb(36,91,219); background-color: inherit">SAM 2</span>](https://arxiv.org/abs/2408.00714) paper** ([<span style="color: rgb(36,91,219); background-color: inherit">our pod</span>](https://latent.space/p/sam2)) - the very successful image and video segmentation foundation model. Pair with [<span style="color: rgb(36,91,219); background-color: inherit">GroundingDINO</span>](https://github.com/IDEA-Research/GroundingDINO).

- **Early fusion research**: Contra the cheap “late fusion” work like [<span style="color: rgb(36,91,219); background-color: inherit">LLaVA</span>](https://arxiv.org/abs/2304.08485) ([<span style="color: rgb(36,91,219); background-color: inherit">our pod</span>](https://www.latent.space/p/neurips-2023-papers)), early fusion covers Meta’s [<span style="color: rgb(36,91,219); background-color: inherit">Flamingo</span>](https://huyenchip.com/2023/10/10/multimodal.html), [<span style="color: rgb(36,91,219); background-color: inherit">Chameleon</span>](https://arxiv.org/abs/2405.09818), Apple’s [<span style="color: rgb(36,91,219); background-color: inherit">AIMv2</span>](https://arxiv.org/abs/2411.14402), Reka [<span style="color: rgb(36,91,219); background-color: inherit">Core</span>](https://arxiv.org/abs/2404.12387), et al. In reality there [<span style="color: rgb(36,91,219); background-color: inherit">are at least 4 streams of visual LM work</span>](https://lilianweng.github.io/posts/2022-06-09-vlm/).

Much frontier VLM work these days is no longer published (the last we really got was [<span style="color: rgb(36,91,219); background-color: inherit">GPT4V system card</span>](https://cdn.openai.com/papers/GPTV_System_Card.pdf) and [<span style="color: rgb(36,91,219); background-color: inherit">derivative papers</span>](https://arxiv.org/abs/2309.17421)). We recommend having working experience with vision capabilities of 4o (including [<span style="color: rgb(36,91,219); background-color: inherit">finetuning 4o vision</span>](https://blog.roboflow.com/gpt-4o-object-detection/)), Claude 3.5 Sonnet/Haiku, Gemini 2.0 Flash, and o1. Others: [<span style="color: rgb(36,91,219); background-color: inherit">Pixtral</span>](https://mistral.ai/news/pixtral-large/), [<span style="color: rgb(36,91,219); background-color: inherit">Llama 3.2</span>](https://buttondown.com/ainews/archive/ainews-llama-32-on-device-1b3b-and-multimodal/), [<span style="color: rgb(36,91,219); background-color: inherit">Moondream</span>](https://www.youtube.com/watch?v=T7sxvrJLJ14), [<span style="color: rgb(36,91,219); background-color: inherit">QVQ</span>](https://news.ycombinator.com/item?id=42505038).

OCR is an important subset workhorse functionality of vision, so you may wish to check out [<span style="color: rgb(36,91,219); background-color: inherit">Mistral OCR</span>](https://news.ycombinator.com/item?id=43282905) and [<span style="color: rgb(36,91,219); background-color: inherit">VLM Run</span>](https://news.ycombinator.com/item?id=43187209).

## Section 8: Voice

- **[<span style="color: rgb(36,91,219); background-color: inherit">Whisper</span>](https://arxiv.org/abs/2212.04356) paper&#x20;**- the successful **ASR&#x20;**&#x6D;odel from Alec Radford. Whisper [<span style="color: rgb(36,91,219); background-color: inherit">v2</span>](https://news.ycombinator.com/item?id=33884716), [<span style="color: rgb(36,91,219); background-color: inherit">v3</span>](https://news.ycombinator.com/item?id=38166965) and [<span style="color: rgb(36,91,219); background-color: inherit">distil-whisper</span>](https://github.com/huggingface/distil-whisper) and [<span style="color: rgb(36,91,219); background-color: inherit">v3 Turbo</span>](https://amgadhasan.substack.com/p/demystifying-openais-new-whisper) are open weights but have no paper.

- **[<span style="color: rgb(36,91,219); background-color: inherit">AudioPaLM</span>](http://audiopalm/) paper&#x20;**- our last look at Google’s voice thoughts before PaLM became Gemini. See also: Meta’s [<span style="color: rgb(36,91,219); background-color: inherit">Llama 3 explorations into speech</span>](https://arxiv.org/abs/2407.21783).

- **[<span style="color: rgb(36,91,219); background-color: inherit">NaturalSpeech</span>](https://arxiv.org/abs/2205.04421?utm_source=chatgpt.com) paper&#x20;**- one of a few leading **TTS** approaches. Recently [<span style="color: rgb(36,91,219); background-color: inherit">v3</span>](https://arxiv.org/abs/2403.03100).

- **[<span style="color: rgb(36,91,219); background-color: inherit">Kyutai Moshi</span>](http://moshi/) paper&#x20;**- an impressive full-duplex speech-text open weights model with [<span style="color: rgb(36,91,219); background-color: inherit">high profile demo</span>](https://www.youtube.com/watch?v=hm2IJSKcYvo). See also [<span style="color: rgb(36,91,219); background-color: inherit">Hume OCTAVE</span>](https://www.hume.ai/blog/introducing-octave).

- [**<span style="color: rgb(36,91,219); background-color: inherit">OpenAI Realtime API: The Missing Manual</span>**](https://www.latent.space/p/realtime-api) - Again, frontier omnimodel work is not published, but we did our best to document the Realtime API.

We do recommend diversifying from the big labs here for now - try Daily, Livekit, Vapi, Assembly, Deepgram, Fireworks, Cartesia, Elevenlabs etc. See [<span style="color: rgb(36,91,219); background-color: inherit">the State of Voice 2024</span>](https://www.cartesia.ai/blog/state-of-voice-ai-2024). While NotebookLM’s voice model is not public, [<span style="color: rgb(36,91,219); background-color: inherit">we got the deepest description of the modeling process</span>](https://www.latent.space/p/notebooklm) that we know of. Historical interest: [<span style="color: rgb(36,91,219); background-color: inherit">wav2vec</span>](https://arxiv.org/abs/2006.11477) (see Snipd pod)

With Gemini 2.0 also being natively voice and vision multimodal, the Voice and Vision modalities are on a clear path to merging in 2025 and beyond.

## Section 9: Image/Video Diffusion

- **[<span style="color: rgb(36,91,219); background-color: inherit">Latent Diffusion</span>](https://arxiv.org/abs/2112.10752) paper** - effectively the Stable Diffusion paper. See also [<span style="color: rgb(36,91,219); background-color: inherit">SD2</span>](https://stability.ai/news/stable-diffusion-v2-release), [<span style="color: rgb(36,91,219); background-color: inherit">SDXL</span>](https://arxiv.org/abs/2307.01952), [<span style="color: rgb(36,91,219); background-color: inherit">SD3</span>](https://arxiv.org/abs/2403.03206) papers. These days the team is working on [<span style="color: rgb(36,91,219); background-color: inherit">BFL Flux</span>](https://github.com/black-forest-labs/flux) \[schnell|dev|pro].

- **[<span style="color: rgb(36,91,219); background-color: inherit">DALL-E</span>](https://arxiv.org/abs/2102.12092) / [<span style="color: rgb(36,91,219); background-color: inherit">DALL-E-2</span>](https://arxiv.org/abs/2204.06125) / [<span style="color: rgb(36,91,219); background-color: inherit">DALL-E-3</span>](https://cdn.openai.com/papers/dall-e-3.pdf) paper&#x20;**- OpenAI’s image generation.

- **[<span style="color: rgb(36,91,219); background-color: inherit">Imagen</span>](https://arxiv.org/abs/2205.11487) / [<span style="color: rgb(36,91,219); background-color: inherit">Imagen 2</span>](https://deepmind.google/technologies/imagen-2/) / [<span style="color: rgb(36,91,219); background-color: inherit">Imagen 3</span>](https://arxiv.org/abs/2408.07009) paper** - Google’s image gen.

- **[<span style="color: rgb(36,91,219); background-color: inherit">Consistency Models</span>](https://arxiv.org/abs/2303.01469) paper** - this distillation work with [<span style="color: rgb(36,91,219); background-color: inherit">LCMs</span>](https://arxiv.org/abs/2310.04378) spawned the [<span style="color: rgb(36,91,219); background-color: inherit">quick draw viral moment of Dec 2023</span>](https://www.latent.space/p/tldraw). These days, updated with [<span style="color: rgb(36,91,219); background-color: inherit">sCMs</span>](https://arxiv.org/abs/2410.11081) or [<span style="color: rgb(36,91,219); background-color: inherit">DMDs</span>](https://github.com/Tongyi-MAI/Z-Image#user-content--decoupled-dmd-the-acceleration-magic-behind-z-image).

- **[<span style="color: rgb(36,91,219); background-color: inherit">Sora</span>](https://openai.com/index/sora/) blogpost** - text to video - no paper of course beyond [<span style="color: rgb(36,91,219); background-color: inherit">the DiT paper</span>](https://arxiv.org/abs/2212.09748) (same authors), but still the most significant launch of the year, with many [<span style="color: rgb(36,91,219); background-color: inherit">open weights competitors</span>](https://artificialanalysis.ai/text-to-video/arena?tab=Leaderboard) like [<span style="color: rgb(36,91,219); background-color: inherit">OpenSora</span>](https://arxiv.org/abs/2412.00131). [<span style="color: rgb(36,91,219); background-color: inherit">Lilian Weng survey here</span>](https://lilianweng.github.io/posts/2024-04-12-diffusion-video/).

- [**<span style="color: rgb(36,91,219); background-color: inherit">Autoregressive image generation</span>**](https://www.youtube.com/watch?v=yJ396Ksiv2s) is all the rage this year for [<span style="color: rgb(36,91,219); background-color: inherit">Gemini</span>](https://developers.googleblog.com/en/experiment-with-gemini-20-flash-native-image-generation/), [<span style="color: rgb(36,91,219); background-color: inherit">4o</span>](https://news.ycombinator.com/item?id=43474112#43475809), and [<span style="color: rgb(36,91,219); background-color: inherit">Llama’s</span>](https://arxiv.org/abs/2406.06525) Native Image Gen.

- More Image/Video work is done in Startups now. See [<span style="color: rgb(36,91,219); background-color: inherit">Ideogram</span>](https://www.reddit.com/r/singularity/comments/1exsq4d/introducing_ideogram_20_our_most_advanced/), [<span style="color: rgb(36,91,219); background-color: inherit">Recraft</span>](https://www.recraft.ai/), [<span style="color: rgb(36,91,219); background-color: inherit">Reve Image</span>](https://buttondown.com/ainews/archive/ainews-halfmoon-is-reve-image-a-new-sota-image/), Pika, Playground, [<span style="color: rgb(36,91,219); background-color: inherit">Genmo</span>](https://github.com/genmoai/mochi) and Chinese models like Wan 2.1 and Kling.

We also highly recommend familiarity with **ComfyUI** ([<span style="color: rgb(36,91,219); background-color: inherit">we were first to interview</span>](https://www.latent.space/p/comfyui)). [<span style="color: rgb(36,91,219); background-color: inherit">Text Diffusion</span>](https://www.youtube.com/watch?v=1mG678f1ZYU&pp=ygUOdGV4dCBkaWZmdXNpb24%3D) ([<span style="color: rgb(36,91,219); background-color: inherit">Mercury/Inception</span>](https://www.inceptionlabs.ai/blog/mercury-refreshed) is SOTA) and [<span style="color: rgb(36,91,219); background-color: inherit">Music Diffusion</span>](https://arxiv.org/abs/2302.03917) are niche for now.

For historical interest: **[<span style="color: rgb(36,91,219); background-color: inherit">DALL-E</span>](https://arxiv.org/abs/2102.12092) / [<span style="color: rgb(36,91,219); background-color: inherit">DALL-E-2</span>](https://arxiv.org/abs/2204.06125) / [<span style="color: rgb(36,91,219); background-color: inherit">DALL-E-3</span>](https://cdn.openai.com/papers/dall-e-3.pdf) paper&#x20;**- OpenAI’s image generation, and **[<span style="color: rgb(36,91,219); background-color: inherit">Imagen</span>](https://arxiv.org/abs/2205.11487) / [<span style="color: rgb(36,91,219); background-color: inherit">Imagen 2</span>](https://deepmind.google/technologies/imagen-2/) / [<span style="color: rgb(36,91,219); background-color: inherit">Imagen 3</span>](https://arxiv.org/abs/2408.07009) paper** - Google’s image gen.

## Section 10: Finetuning

- **[<span style="color: rgb(36,91,219); background-color: inherit">LoRA</span>](https://arxiv.org/abs/2106.09685)/[<span style="color: rgb(36,91,219); background-color: inherit">QLoRA</span>](http://arxiv.org/abs/2305.14314) paper** - the de facto way to finetune models cheaply, whether on local models or with 4o ([<span style="color: rgb(36,91,219); background-color: inherit">confirmed on pod</span>](https://www.latent.space/p/cosine)). [<span style="color: rgb(36,91,219); background-color: inherit">FSDP+QLoRA</span>](https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html) is educational.

- **[<span style="color: rgb(36,91,219); background-color: inherit">DPO</span>](https://arxiv.org/abs/2305.18290) paper&#x20;**- the popular, if slightly inferior, alternative to [<span style="color: rgb(36,91,219); background-color: inherit">PPO</span>](https://arxiv.org/abs/1707.06347), now supported by OpenAI as [<span style="color: rgb(36,91,219); background-color: inherit">Preference Finetuning</span>](https://platform.openai.com/docs/guides/fine-tuning#preference).

- **[<span style="color: rgb(36,91,219); background-color: inherit">ReFT</span>](https://arxiv.org/abs/2404.03592) paper** - instead of finetuning a few layers, focus on features instead.

- **[<span style="color: rgb(36,91,219); background-color: inherit">Orca 3/AgentInstruct</span>](https://www.microsoft.com/en-us/research/blog/orca-agentinstruct-agentic-flows-can-be-effective-synthetic-data-generators/) paper** - see the [<span style="color: rgb(36,91,219); background-color: inherit">Synthetic Data picks at NeurIPS</span>](https://www.latent.space/p/2024-syndata-smolmodels) but this is a great way to get finetue data.

- **RL/Reasoning Tuning papers** - [<span style="color: rgb(36,91,219); background-color: inherit">RL Finetuning for o1</span>](https://www.interconnects.ai/p/openais-reinforcement-finetuning) is debated, but [<span style="color: rgb(36,91,219); background-color: inherit">Let’s Verify Step By Step</span>](https://arxiv.org/abs/2305.20050) and [<span style="color: rgb(36,91,219); background-color: inherit">Noam Brown’s many public talks</span>](https://x.com/swyx/status/1867990396762243324) give hints for how it works.

We recommend going thru [<span style="color: rgb(36,91,219); background-color: inherit">the Unsloth notebooks</span>](https://github.com/unslothai/unsloth) and HuggingFace’s [<span style="color: rgb(36,91,219); background-color: inherit">How to fine-tune open LLMs</span>](https://www.philschmid.de/fine-tune-llms-in-2025) for more on the full process. This is obviously an endlessly deep rabbit hole that, at the extreme, overlaps with the Research Scientist track
